# MongoDB权威指南 #

# 简介 #
没有预定义模式, 文档的键值不再是固定的类型和大小, 这样添加或删除字段变得更容易.

## 功能 ##
* 索引
* 聚合
* 特殊的集合类型
* 文件存储

# 第2章 MongoDB基础知识 #
- 文档是基本单元
- 集合看做是一个拥有动态模式的表
- 一个MongoDB的实例可以有多个相互独立的数据库 每个数据库拥有自己独立的集合
- 每个文档都有一个特殊的键叫做 "_id" 这个键在文档所属的集合中是唯一的
- MongdoDB自带了一个简单但强大的类似JS的shell, 可以用于管理数据库

## 文档 ##
类似 json字符串
**注**: 两个文档相等: 键的顺序相等 + 键值类型相等  + 键值值相等

## 集合 ##
### 动态模式 ###
指的是一个集合里的文档不一定具有相同的模式

### 集合命名 ###
有些特殊字符不要使用
有些特定的前缀不要使用

### 子集合 ###
用 . 隔开 比如 blog.authors
仅仅起到一种组织的作用
blog 集合和 blog.authors 集合 可能完全没啥关系

## 2.3 数据库 ##
一个数据库可以有多个集合, 一个MongoDB实例可以有多个数据库

### 命名 ###
见原文

### 保留的数据库名 ###
admin local config

## 启动MongoDB ##
默认占用27017 并且开放一个简单的http服务器 在 27017+1000=28017端口

mongo.exe
db指向了当前连接的数据库
切换数据库 `use databaseName`

### shell中的基本操作 ###
创建
db.users.insert({'name':'xzc','age':20});
读取
db.users.find(...);
更新
db.users.update(...);
删除
db.users.update(remove);

## 数据类型 ##
类似json
json只有 : 6种数据类型 : null 布尔 数字 字符串 数组 对象
而MongoDB的文档在json的基础上规定:
- null
	- 用于表示控制 或者 不存在的字段
- 布尔
	- true 或 false
- 数值
	- 默认使用64位浮点型
	- 如果有需要可以: NumberInt('3') NumberLong('3') 强制指定类型(即常见的int和long类型)
- 字符串
- 日期
	- 日期被保存为从新纪元以来经过的毫秒数
- 正则表达式
	- 语法和js的一样
- 数组
- 内嵌文档
- 对象id
- 二进制数据
- 代码

## MongoDB shell ##
mongo localhost:27017/xxDB
- db指向了当前连接的数据库
- help命令查看帮助
- 可以使用db.help()查看数据库级别的帮助
- 可以使用db.users.help()查看集合级别的帮助
- 可以使用db.users.insert来查看该函数的定义(js的函数)

### 使用脚本 ###
将代码先写好保存在 abc.js 里
然后 mongo abc.js abc2.js abc3.js 就会依次执行 然后退出
mongo --quiet localhost:27017/foo a.js b.js c.js
也可以使用调用  load('a.js') 执行脚本

#### 辅助函数 ####
use foo 切换数据库
show dbs
show collections
load('a.js') 执行脚本
run('ls', param1, param2,...) 执行命令行程序

在当前用户的主目录下创建 ".mongorc.js" 文件
这个文件会在shell启动时候执行
设置EDITOR环境变量
然后 var a=123;
edit a;就会打开你指定的EDITOR程序来进行编辑, 有时候很方便!
当你想执行一段代码的时候, 你可以:
edit a;//这里的a是一个随便的变量名, 等一下要用作函数名的.
然后在记事本里写
function(){
	//你的代码...
}
然后保存退出
在shell里执行a();就行了

见P25

# 第3章 CRUD #
## 批量插入 ##
db.foo.batchInsert(obj1,obj2,...)
如果obj2插入失败, 那么obj2及其之后的文档全部插入失败, 之前的插入成功(可以设置一个选项使得即使出错了也继续插入)
特殊情况还需要考虑最大文档的限制

### 插入校验 ###
如果没有_id 就自动生成一个
素有文档的大小有一定限制
可以使用Object.bsonsize(doc)查看一个文档的大小

db.mailing.list.remove({'opt-out':true});
删除mailing.list集合中opt-out=true的文档

## 删除 ##
如果是要删除所有数据的话 用drop会更快

## 更新 ##
分为update和upsert
upsert在没有匹配文档的时候, 会根据update的条件创建出一个新文档
并且会用到大量的修改器
注意mongodb具有文档级别的原子性, 所以很多时候upsert很有作用
有的时候, 需要在创建文档的时候同时为这个字段赋值, 那么需要使用$setOnInsert
update({},{$setOnInsert:{"updateAt":new Date()}}, true);
第一个参数为空, 表明要更新所有文档
第二个参数里使用了$setOnInsert, 表明只有在创建新记录的时候才会使用这个东西

### 如果需要更新多个文档需要特殊指定 ###


## save ##
就是createOrUpdate操作

## 修改器 ##
$set $unset 设置某个属性或删除某个属性
$inc 只能用于整数 长整数 双精度浮点型 其他的类型会导致失败
$push
	{"$push":{key:value}} 往key数组末尾添加一个元素
	{"$push":{key:{"$each":[value1,value2,value3]}}} 一次性加入3个元素
	{"$push":{key:{"$each":[value1,value2,value3],"$slice":-10}}} 一次性加入3个元素, 并且保留最后的10个元素
	{"$push":{key:{"$each":[value1,value2,value3],"$slice":-10,"$sort":{"score":-1}}}} 一次性加入3个元素, 并且保留最后的10个元素, 元素的顺序是按照score以升序排列的最后10个元素
	$slice必须和$each一起使用
$pop
	$pop:{"key":1} 从末尾删除元素
	$pop:{"key":-1} 从头部删除元素
$pull	
	{"$pull":{"array":"xzc"}}删除array数组中的所有xzc元素

$ne
$addToSet

子操作符
$each
$slice 必须要和$each和$push一起用
	必须是负整数 : 只保留数组最后的n个元素
$sort 必须要和$each和$push一起用
	
## 数组的相关操作 ##
假设user集合里面放了很多的user, 每个user可以有多个qq, 用一个qqs数组来存放qq号
{"qqs":{"$ne":"70862045"}} qqs数组里不包含"70862045"
{"$addToSet":{"qqs":"70862045"} 把qqs当做集合对待, 这样可以保证元素的唯一性
{"qqs":{"$addToSet":{"$each":["qq1","qq2"]}} 把qqs当做集合对待, 这样可以保证元素的唯一性
对数据的第一个元素进行操作
$inc:{comments.0.score:1} comments是一个数组, 将它的第一个元素(下标从0开始)的score加上1
update({'comments.author':'A'},'{"$set":{'comments.$.author','B'}}')$set: {comments.$.author, 'Jim'} 将所有A的回复的回复者都设置成B, update的第一个参数是要更新的条件, 这里comments是一个数组, 意思就遍历这个数组, 选中所有author='A'的项; 然后第二个参数是要做的更新操作, 是一个set操作, 但这个set操作由于是要修改数组里面的项, 因此需要有一个索引, 而这个索引就用$来代替; 定位符只更新第一个匹配的元素, 如果A有多个评论, 那么只会在第一条评论中改变


## 修改器的速度 ##
$inc 原地修改 比较快
$set 能在文档大小不发生改变的时候立即修改文档
将数据插入db时, 数据是相邻存放的, 因此如果第二个数据变得太大了, 那么它就不可能放在原来的位置, 而是要移动到别的位置, 这样检索数据的时候就会慢
有一个填充因子, 可以控制每个文档的预留空间
如果$push操作是瓶颈的话, 那么可能需要考虑将该数组化作一个集合来管理
重新整理分配控件
db.runCommand({'collMod':<CollectionName>, 'usePowerOf2Sizes':true})

# 3.4 写入安全机制 #
这是一个客户端设置, 默认情况下, 很多操作都阻塞直到数据库响应, 这样就有点慢了.
应答式写入(默认) 非应答式写入
后者会直接返回, 然后在后台发出请求, 因此无法知道写入是否从成功
也因此对于一些不关心的数据可以用后者


# 4.1 find简介 #
需要提供 "需要满足的条件" 和 "需要返回的字段"
{age:11} 满足age=11
{name:'xzc'} 满足name='xzc'
{age:11, name:'xzc'} 满足name='xzc' 且 满足age=11
第二个参数用于指定哪些字段需要被返回, 如果没有指定的话默认是返回全部字段
可以用 {key:1} 显示指定要返回key字段, key:0表示不要返回该字段
在使用了key:1的情况下, 其他没指定为1的字段都不返回
在使用了key:0的情况下, 只有指定为0的才不返回
如果不显示指定_id:0, 那么_id是会返回的
### 查询条件 ###
$lt $lte $gt $gte $ne
{age:{$gte:18, $lte:30}} 18<=age<=30
$in $nin
{age:{$in:[1,2,3]}} 找出age in [1,2,3]的
$or
{$or:[{age:1},{age:2}]} 就是对$or对应的数组进行or操作
and的第一个条件尽量匹配少的结果, (and语句并不需要特殊指定, 默认就是了)
or的第一个条件应该尽量匹配多的结果

age:{$not:{$mod:[5,3]}} 所有年龄模5不为3的
$exists 用于判定键值是否存在
如果你想找出所有age=null的文档
你的条件如果这样写是没用的: {age:null} 因为这和{}是一样的
如果确实像找出来, 那么需要使用
age:{$in:[null],$exists: true}
前面的$in是因为没有一个操作符叫做$eq (真没有吗?), 又不能简单的使用age:null, 而采用的一种策略
$exists是确保这个键是存在的


$not $and $or $nor 位于外层
$lt位于内层 条件语句是内层键
$inc位于外层 修改器是外层键

## 正则表达式 ##
find({"name": /xzc/i})
/xzc/i 是一个人正则表达式
但是注意, 如果"name"对应的内容本身是一个正则表达式, 那么也会匹配成功, 虽然几乎没有人会这么做

## 查询数组 ##
假设qqs是一个数组
qqs:"70862045", 只要qqs里包含一个70862045就行
qqs:["70862045","8147532"], qqs必须和指定的数组完全相同, 元素和顺序都要正确!
qqs:{$all:["70862045","8147532"]}, 只要qqs两个都包含就行
可以使用 key.index 语法访问特定的元素
$size用来查询特定长度的数组
qqs:{$size:3} 找出那些qqs里有3个元素的文档
但是这个$size没法进行一些其他的比较操作 比如大于
因此可以考虑对一个数组, 增加一个新的字段来表明这个数组的长度
但是在堆这个数据进行添加或删除的时候也要更新一个这个字段
这样对性能几乎没有影响
$slice:10 返回前10个元素
$slice:-10 返回后10个元素
$slice:[10,5] offset,size 似乎offset也支持负数
{comments:{$slice:-1}} 返回comments的最后一个元素
只返回数组中符合条件的内嵌文档
find({comments.author:'A'},{'comments.$':1})
第二个元素将comments.$设置成1 这样在最终的结果就会包含满足条件的内嵌文档, 但是最多只会返回一个元素

$elemMatch 只要数组里的某个元素满足$elemMatch提出的条件 那么就通过

## 查询内嵌文档 ##
查询内嵌文档的时候需要注意顺序
假设friend放了一个好朋友的文档
find({friend:{name:'xzc', age: 20}}) 那么必须保证该子文档完全匹配, 包括元素和顺序
find({'friend.name':'xzc','friend.age':20}) 这样是比较推荐的作为
点.可以认为是进入内嵌文档
因此键名不可以包含点号

只要comments数组里存在一个元素满足 author=xzc age<20 即可
find({comments:{$elemMatch:{author:'xzc', age:{"$lt":20}}}})

## 查询的不足 ##
查询的时候使用的value, 只能是你程序提供的而不是数据库里有的
	比如我们想要找出 所有满足 key1 = key2 的文档
	但是你没法指定

## $where登场 ##
$where几乎万能 但是效率没有一些专门的查询条件高

## 游标 ##
sort skip limit
要尽量少用skip, 因为skip需要临时保存数据, 然后再跳过这些数据
很多时候是不够快的, 如果你的分页是有依据的话, 那么就可以依赖于上一次分页的结果来确定下一页的查询 从而避开使用skip
当sort时, 某个字段对应的类型不是固定的时候, 排序顺序是预先定义好的

### 随机选取文档 ###
有的时候我们需要随机选取一个文档进行展示, 当然你可以使用skip.
但如果很经常这么做的话, 推荐:
	给每个文档新加一个字段random, 存在一个0~1的随机数
	然后find({random:{"$gt":生成一个随机数}})
	然后这需要确保随机键的有索引

### 高级查询 ###
$maxscan:integer
	每次查询中扫描文档数量的上限
$min/$max:document
	指定扫描的范围
$showDiskLoc: true
	显示该文档在磁盘上的位置

### 获取一致的结果 ###
对同一个查询, 如果没有用sort来指定顺序的话, 那么返回的顺序是不确定的
而且还可能出现这种情况, 本来数据库中只有100个元素, 你采用迭代器遍历的方式进行遍历, 遍历过程中你可能会修改一些元素, 因为你修改了一些元素, 可能导致元素变大, 然后在数据库原有的位置放不下, 然后这个元素就会被安排新的位置, 比如放在最后一个元素的后面, 因此你可能会遍历到超过100个的元素

### 游标生命周期 ###
当一个游标被遍历完之后或客户端主动请求就会被关闭
如果一个游标若干分钟没有使用那么也会被关闭

## 数据库命令 ##
用于管理数据库本身
db.runCommand({'drop':'test'})
和辅助函数db.test.drop()效果是一样的, 这个辅助函数是对上面的封装



# 第5章 索引 #
db.users.ensureIndex({...},{选项})创建索引
选项可以指定是否创建唯一索引, 稀疏索引, 索引的名字
dropIndexs()
可以根据这个集合经常用来当做依据的查询的字段和排序关键字来作为索引

db.users.find(...).explain()就可以看到本次查询的一些指标
比如花费时间, 扫描的文档的数量和返回的结果个数

当一个索引覆盖了用户本次查询请求的所有字段就称这个索引覆盖了本次查询, 应该优先使用覆盖索引
在覆盖索引上执行explain会有indexOnly=true

## 索引基数 ##
指的是集合中某个字段拥有的不同值的数量.
比如性别就只有男,女,保密.
但其他一些值就不是这样了, 必须姓名, 很大
使用explain和hint方法进行诊断

# 第6章 特殊的索引和集合 #
固定集合: 总大小固定或者最大数量固定
	一旦哪一个值被超过了, 那么最旧的元素就会被删除
db.createCollection('集合的名字',{capped:true,size:10000, max:100})
最大大小为10000字节, 元素最多100个的 固定集合
size必须制定 max可以不指定
固定集合不能转成普通集合, 但是普通集合可以转成固定集合
循环游标: 一个游标被遍历完后并没有被关闭, 你可以时不时的对它hasNext()一下, 如果为true那就说明又有了新数据了

TTL索引
db.users.ensureIndex({...},{expireAfterSecs:3600})那么这个文档3600秒后就会过期
MongoDB每分钟对TTL索引进行一次清理

# 第7章 聚合框架 #
db.users.aggregate({...},{...},{...})
$sort $limit $skip $group
管道 筛选 投射 分组 排序 限制 跳过
$match 对文档集合进行筛选
	$match里可以使用常规的查询操作符
$project 可以从子文档中提取字段 可以重命名字段 表达式计算
	$project:{userid:"$_id"}
	用$FieldName 就可以新建一个字段userid,它的值等于_id的值
	而且使用这种技术就可以生成一个值的两个副本 在某些时候有用
	$add:[exp1[, exp2 ...] ]
	$subtract $mod $divide $multiply
	日期相关的表达式
	$month $week $dayOfMonth ...
	字符串相关表达式
	逻辑表达式
	$cmp $or $eq $and ... $ifNull
	
### 分组 $group ###
用_id指定分组的依据, 可以复合
$sum: expr
	将组里的每个文档的expr表达式的结果进行相加
$avg $min $max $first $last
	这里的min和max会查看每个文档, 因此如果已经有序的话推荐使用first和last
	
数组操作
$addToSet $push

#### $unwind ####
拆分可以将数组中的每一个值拆分成单独的文档

### 建议 ###
1. 在管道开始阶段尽量 排序 刷仙 unwind group等操作

## MapReduce ##
MR有点慢, 不适合实时的数据分析
其中reduce阶段要能够被重复执行
map阶段发射的 和 reduce阶段返回的必须结构要相同
MR的过程可能是这样的:
将A个数据, 分别进行map操作, 每个map操作可以发射出若干个obj, 然后假设总共发射出B个obj, 然后这B个obj会被交给reduce函数, 但并不是一次性交给, 比如说我先拿100个去给reduce函数(这样的好处是不用等全部的对象都发射完毕才开始进行reduce操作), 由于reduce函数又会给我返回一个和给它的obj具有相同结构的obj, 这样相当于就减少了99个obj了. 然后你会发现这个模式的话, 很容易进行并行的运算. 反正合并到最后就会只剩下一个obj, 那就是最后的结果.

### Mongodb的MapReduce ###
新增了一些可选的键
finalize:function
	可以将reduce的结果发送给这个键, 这是整个处理过程的最后一步
query:document
	用于发给map函数的数据的筛选
	这样MR的输入就不再是一整个集合了
limit:integer
	限制发给map函数的文档数量
score:document
	用obj的方式给map和reduce提供一个上下文变量
其他的见书本P146

## 简单的聚合命令 ##
count
distinct
group
	需要提供集合名 和 一个过滤条件(可选)
	需要提供一个键的名或一个函数可以返回键值
	然后提供一个初始值(用作每一个组的初始值), 然后提供一个reduce函数
	可以提供一个finalize函数对每组的结果进行最后的处理
	ns 集合名 字符串
	key 键, 字符串 或 对象 使用{'name':true, age:true} 让两个字段作为key
	$keyf:function(doc){返回这个文档的key}
	$reduce:function(doc,prev){对prev进行修改即可}
	finalize:function(prev){最后的修改}

# 第9章 创建副本集 #
## 建立副本集 ##
启动 mongod 的时候 使用 --replSet <NAME> 来指定一个副本集的名字
想要在同一个副本集里的mongod要使用相同的副本集名字
假设你开了3台服务器, s1,s2,s3, 此时他们还互相不知道对方
然后连接到其中任意一台服务器上, 比如s2
然后你执行:
```
var config={
	"_id" : "你在<NAME>上写的名字",
	"members":[
		{"_id":0, "host":"1.1.1.1:11111"},
		{"_id":1, "host":"2.2.2.2:22222"},
		{"_id":2, "host":"3.3.3.3:33333"}
	]
};
rs.initiate(config);
然后这台服务器就会根据这个配置主动去联系其他的服务器, 然后完成配置, 完成配置之后, 其中一个成员会自动成为主节点(不一定是你当前连接的服务器)
如果你不是在主节点, 那么你的提示符就会有类似 xzc:OTHER> 其中OTHER表明了你现在在本分节点
可以使用
db.isMaster()查看详细信息
```
通过rs.config()可以拿到你的当前配置
在副本集的任意一个成员上可以执行:
rs.add('4.4.4.4:44444'); 添加新的成员
rs.remove('4.4.4.4:44444'); 添加成员
执行rs.config();会返回当前的配置对象, 可以打印看看就知道
你可以在这个对象的基础上做一些修改, 修改完后在执行:
rs.reconfig(config);

## 副本集的设计 ##
### "大多数"的概念 ###
使得人数超过半数, 总人数指的是该副本集里的成员个数
即使该成员现在连接不上, 也要算它的人数.
### 投票机制 ###

## 成员配置 ##
### 仲裁者 ###
以dbpath="一个空的目录", 并且制定replSet, 启动一台服务器, 然后在我们的当前数据集的shell上执行rs.addArb, 或提供config的时候就制定arbiterOnly=true这个选项奶个你
仲裁者本身不保存数据, 只是在投票的时候起作用, 当然了, 它本身肯定不能被选为主节点的
rs.addArb('1.1.1.1:11111')进行添加
或
rs.add({"_id":4, "host":"...", 'aibiterOnly' : true});
一旦一个成员以仲裁者的身份加入这个副本集, 那么它的身份就不能改变
除非先删除了再加入
一个副本集中只能有一个仲裁者
### 缺点 ###
1. 活生生浪费了一个服务器, 当然如果本服务器本身有其他用途, 它只是顺便作为一个仲裁者的话, 那就不算是缺点

### 优先级 ###
rs.add({"_id":4, "host":"...", priority:2 });
或手动拿到config对象之后
config.members[0].priority=2, 之后记得要执行rs.reconfig(config)操作
1. 默认的优先级是1, 越高越容易成为主节点
2. 优先级为0不能成为主节点

### 隐藏成员 ###
1. 隐藏成员不能作为其他成员的同步源, 客户端也不会向隐藏成员发送请求
2. 常用于能力不强的服务器
3. 指定一个参数 hidden=true, 同时要有priority=0
4. 可以将一个隐藏成员设置成非隐藏的, 只要将hidden=false, 再reconfig一下就行了

### 延迟备份节点 ###
1. 故意让节点的备份延迟一些时间
2. 使用slaveDelay参数, 单位是秒
3. 备份节点必须有 priority=0
4. 如果你的应用程序会将读请求路由到备份节点, 那么应该将备份节点隐藏掉

### 创建索引 ###
1. 备份节点并不需要与主节点拥有相同的索引
2. 甚至可以没有索引
3. 使用buildIndexs参数来表示是否要创建索引
4. 一旦指定了false, 就无法修改成true
5. 如果确实要这么做, 先退出副本集, 然后删除自己所有数据, 再加入副本集
6. 要求priority=0
	
# 第10章 副本集的组成 #
1. 操作日志记录了主节点(oplog)的每一次写操作, 这些日志会被记录到一个集合, 备份节点可以通过查询这个集合来知道需要进行复制的操作.
2. 每个备份节点都有自己的oplog, 记录着每一次从主节点复制数据的操作.
3. 这样每个成员都可以作为同步元提供给其他成员使用
4. 备份节点从某一个同步元中选区获取执行的操作


## 初始化同步 ##
当副本结的成员启动之后, 会检查自身状态, 然后从某个成员那里进行同步; 如果不能同步的话, 它会尝试从副本的另外一个成员那里进行完整的数据复制.

1. 选择一个成员作为同步元, 在local.me中为自己创建一个标识符, 删除自己所有的数据库, 以一个全新的状态进行同步
2. 将同步元的所有记录复制到本地
3. 进行oplog同步, 此后本地的数据库与主节点的某个时间点的数据集完全一致. 创建索引
4. 如果当前节点远远落后同步元, 那么还会再...
5. 当前成员已经
6. 完成了初始化同步, 切换到普通状态, 这是该当前成员就可以成为备份节点了.

## 心跳 ##
每个成员将自己的心跳发给其他成员来表示自己的状态
心跳间隔2秒
一旦某个备份节点没法连接到主节点, 他就会向成员们申请成为主节点, 其他的成员就会进行投票, 有比较多的评估指标, 需要获得大多数的成员的支持.

# 第11章 从客户端连接到副本集 #
1. 首先地址要这样写: mongodb://server-1:27001,server-2:27002
2. 这里并不需要列出所有的成员的地址, 只需要列出一两个你认为可以连接到的服务器的地址就行了
3. 连接上了之后它会自动获取当前的所有副本集的成员地址
4. 可以选择将读请求路由到备份节点, 如果对数据一致性要求不高的话

## 等待写入复制 ##
意思是, 本次的写操作至少要写入到多少个服务器(包括主节点和备份节点)才能成功返回
同时还可以指定一个超时时间
想想一下: 你将一个数据写入到主节点, 然后还来不及同步到其他的备份节点, 主节点就崩溃了
此时别的节点会被选为主节点, 但是这个主节点并没有你写入的数据
然后等原来那个主节点复活了之后, 它的数据会跟当前的主节点发生冲突, 它会不得不回滚, 你的这个数据会被保存到特殊的回滚文件中, 你需要手动进行处理才能恢复.
此时如果你是将写入到1个以上的服务器, 就可以解决这个问题了
见P11.2附近, WriteConcern可以指定要写入到多少服务器才算是成功, 是否同步, 是否写入日志, 超时时间
可以自定义策略
首先给每个成员新建一个字段 tag
var config = rs.config();
config.members[0].tags={"dc":"shanghai"};
config.members[1].tags={"dc":"shanghai"};
config.members[2].tags={"dc":"beijing"};
config.members[3].tags={"dc":"beijing"};
config.members[4].tags={"dc":"beijing"};
这样5台服务器就被分成了2组, 上海的一组, 北京的一组
并且在副本集中创建getLastErrorMode字段
config.settings={};
config.settings.getLastErrorModes={"eachDC":{"dc":2}}
上面这句话的意思是当采用eachDC策略的时候, 必须保证写入操作真的写到2组不同的dc里才算成功
也就是说上海和北京要至少有一台服务器被写入数据才算成功
```
		WriteConcern wc = new WriteConcern( "xzc" );
		MongoClientOptions mco = new MongoClientOptions.Builder().writeConcern( wc ).build();
		MongoClient client = new MongoClient( Arrays.asList( sa2 ), mco );
```

## 从备份节点读取数据 ##
设置一个 "读取首选项"
支持的选项
1. 主节点优先
2. 低延迟优先
3. 备份节点
4. 备份节点优先

使用场景
1. 一致性要求不高
2. 读取量非常大

有的时候你可能需要做一些离线统计, 做这个统计的时候数据不要求是最新的, 但是平时使用这些数据的时候又要求是最新的
这时候你就可以主动指明我要连接到特定的备份节点, 而不是选择某种策略让驱动帮你选
并且你可以在备份节点建立一些索引, 以提高你做统计的速度, 而其他节点并不一定要有这些索引, 以提高他们的速度
这个特定的备份节点一般有priority=0

# 第12章 管理 #
## 以单机模式启动成员 ##
以单机模式启动一个成员, 做一些维护工作, 然后再让它成为普通的副本集成员

假设我要对某台服务器进行维护, 首先我先关闭它的mongod
然后以 mongod --port=一个别的端口 --dbpath=... 其他照样指定参数
但是不指定 --replSet=...
这样这台服务器就启动了, "一个别的端口"是为了保证不会有其他人连接到这台服务器上

## 副本集配置 ##
副本集配置总是一个文档的形式保存在 local.system.replSet 中
所有的副本集成员的这个文档是相同的, 千万别对它进行修改, 而是要使用一些辅助函数

### 创建/配置副本集 ###
var config = ...
rs.initiate(config)
rs.reconfig(config,{force:true}) force=true使得就算是在非主节点也能进行该配置, 否则可能不会配置成功, 不过强制配置的话可能会出现故障


### 添加/删除 ###
rs.add("ip:port")
rs.add({各种参数}); 可以参考 rs.config.members[0] 的格式
rs.remove('ip:port')


### 副本集有成员数量限制 ###
1. 最多拥有12个成员
2. 其中只有7个成员具有投票权力
3. 这个是目前的 以后可能会修改
4. 当成员是数量大于7的时候, 由于只能有7个成员具有投票权力, 需要将某些成员的投票数量设置成0, 使用votes参数

## 修改成员状态 ##

### 将主节点变为备份节点 ###
连接到主节点的mogno上, 然后执行
rs.stepDown(600) 这样会使得这个节点在600秒内不会成为主节点
600不写的话默认是60

### 阻止选举 ###
连接到主节点
rs.freeze(600) 在600秒内部能进行选举
rs.freeze(0) 允许选举

### 维护模式 ###
通过replSetMaintenanceMode命令
具体不太清除

## 状态 ##
rs.status()以当前服务器的视角查看各个成员的状态
uptime 这个成员从可达一直到现在所经历的时间
optimeDate 这个成员oplog中最后一个操作发生的时间
lastHeartbeat 最后一个心跳时间
pingMs 这个成员与当前服务器的平均ping的时间
errmsg 错误信息
syncingTo 表示该成员正在和哪个同步源

## 自动复制链 ##
连接到某个服务器
db.adminCommand({'replSetSyncFrom':'ip:port'});
然后这个服务器此后就会从这个ip:port进行同步了
直到ip:port坏掉
或使用rs.syncFrom()函数

如果手动指定同步源 那一定要确保不会发生循环
如果是自动复制链的话是不用考虑这个问题的
db.printSlaveReplicationInfo()

## 监控复制 ##
### 调整oplog的大小 ###
1. 先关闭这台服务器
2. 以单机模式打开这台服务器
3. 将oplog的最后一条i记录保存下来, 建议将这条记录保存到一个其他的临时的集合var last = db.oplog.rs.find({'op':'i'}).sort({'$natural':-1}).limit(1).next(); db.myTempCollection.insert(last);db.myTempCollection.findOne()
4. 将oplog集合drop掉 db.oplog.rs.drop()
5. 创建一个新的oplog集合, 它是一个固定集合, 此时你可以指定它的大小 db.createCollection('oplog.rs',{capped:true, size:多少字节});
6. 将刚才那条i记录写入oplog集合 db.oplog.rs.insert(last), 这里的last你要从那个临时的集合里重新读取出来
7. 重新加入副本集
8. 注意要使用findOne来确保数据已经写入到磁盘了

### 从延迟备份节点进行恢复 ###
假设你的数据库不小心删掉了, 由于会进行同步, 你的所有备份节点也被删除了
但是还好 延迟备份节点还有1天以前的数据
这时候你可以将除了延迟备份节点外的所有的服务器都关闭, 然后删除它们数据目录下的所有文件, 然后重新建立加入副本集, 然后他们就会开始恢复数据, 但是不要同一时间启动所有服务器, 不然同步的压力会很大

另一种方法是直接放 延迟备份节点的数据目录整个复制过去
这样回答熬制所有服务器与延迟备份节点具有相同的oplog大小

## 创建索引 ##
在所有服务器上建立索引
方法1:
1. 在主节点上建立索引, 然后这个行为就会扩散到其他节点
2. 这可能到时一段时间都没有服务器可以相应你的请求, 因为他们都在创建索引中
方法2:
1. for(所有备份节点服务器){关掉;以单机模式启动;建立索引;重新加入副本集;}
2. 对主节点特殊处理, 可以直接对主节点创建索引, 这会导致主节点服务器短时间内很忙; 让主节点退下成为备份节点, 然后它再建立索引, 这是利用了选举机制, 代价就是好像主节点挂了一样;

在某个服务器上创建特定的节点, 而其他服务器没有
1. 关闭服务器
2. 以单机模式启动
3. 创建索引
4. 以优先级=0加入副本集

### 纯备份的节点 ###
就是为了纯粹的备份功能而已, 适用于性能不强的服务器
1. priority = 0 hidden=true buildIndexes=false votes=0

### 主节点如何跟踪延迟 ###
local.slaves集合保存了以它为同步源的服务器的信息
查看这个集合可以看到其他服务器的一些信息


# 第13章 分片 #
mongod
mongos
客户端

sh.status()

对数据库进行分片
sh.enableSharding("msg") 对msg数据库启动分片
对一个集合进行分片
use msg;
db.users.ensureIndex({"name":1}); 为name创建索引
sh.shardCollection("users",{"name":1}) 对msg数据库的users集合启动分片, 以name作为片键

## config数据库 ##
在mongos上有一个config数据库

config数据库下的chunks集合保存了各个块的信息
1. 块id
2. 范围


## 均衡器 ##
均衡器负责数据的前移, 它会定期检查分片之间是否存在不平衡, 如果存在就会开始块的前移, 每个mongos有时也会扮演均衡器的角色.
每隔几秒钟mongos就尝试变身为mongos
它会使得每个分片拥有相当的块

# 第14章 #
## 启动顺序 ##
1. 配置服务器
mongod --configsvr --dbpath=... -f=...
--configsvr的功能只是修改默认的端口为27019, 没有其他作用, 但还是推荐加上它 以表明它是一个配置服务器

# 第15章 选择片键 #
片键是集合拆分的依据, 需要考虑一下问题
1. 计划做多少个分片
2. 分片是为了减少读写延迟吗
3. 分片是为了增加读写吞吐量吗
4. 分片是为了增加系统资源吗

## 数据分发 ##
拆分数据最常用的数据分发方式:
1. 升序片键
	1. 如果你的key是递增的, 那么每次插入都会在包含最大key的那个块进进行插入, 导致块不平衡
2. 随机分发的片键
	1. 你的key是具有随机性, 比如用户名, 邮箱, uuid, hash值
	2. 这样可以数据期望是均匀分布的
3. 基于位置的片键
	1. 位置可以是ip, 经纬度等

### 基于位置的片键 ###
以基于ip的片键为例子, 主要是要做到 特定范围的ip出现在特定的分片中
我们为每一个分片添加一个tag, 然后指定为块相应的tag
比如我们想要1.0.0.0-2.0.0.0的ip出现在分片1中
sh.addShardTag('分片1','ShangHai')
sh.addShardTag('分片1','ZheJiang')
sh.addShardTag('分片2','BeiJing')
sh.addTagRange('msg.users',{'ip':'1.0.0.0'},{'ip':'2.0.0.0'},'ShangHai')
这样msg.users里的文档, 只要其ip介于'1.0.0.0'~'2.0.0.0'(注意他们这里是字符串的比较), 那么这个文档就会被放到具有'ShangHai'这个tag的分片里

sh.addTagRange('msg.users',{'ip':'3.0.0.0'},{'ip':'4.0.0.0'},'BeiJing')
这会使得具有ip介于3~4的文档被放到具有BeiJing这个tag的分片里

## 片键策略 ##
### 散列片键 ###
如果追求数据加载速度的极致, 主要是支持快速的随机访问, 那么散列片键非常好, 但是不适用于范围查询, 切记切记.


```js
use msg;
db.users.ensureIndex({name:'hashed'});
sh.shardCollection('msg.users',{name:'hashed'});
```


### 流水策略 ###
如果某些服务器非常强大, 那么我们就期望它有更多的浮在.
假设分片1是用的是ssd, 假设你现在的key就是默认生成的_id
我们给分片1添加一个tag
sh.addShardTag('分片1','ssd')
sh.addTagRange('msg.users',{_id:ObjectId()},{_id:MaxKey},'ssd')
这会使得msg数据库下的users集合, 从这条语句执行后的所有文档都被写入到具有ssd这个tag的分片上, 因为这些分片有ssd, 所以写入速度非常快, 因此可以担此重任
可能过了1天之后, 这些分片上的数据非常多了, 这时候再次执行范围:
```
use config;
var tag=db.tags.findOne('ns':'msg.users');
tag.min.shardKey=ObjectId();这里更新了minKey
db.tags.save(tag);
```
此后旧的数据就不必再放在ssd上了,将会被移到别处

### 多热点 ###
假设我有10台服务器, 我对users集合建立一个复合索引{sid:1,_id:1}
并且用以这个符合索引作为片键
其中sid是一个0~9的随机数, _id是一个递增的主键
每插入一个users, sid起主导作用, 于是所有的users大概会被平均分到10个块上(当数据足够多)
然后在每个块上, 这个块上的文档的sid几乎是相等的
于是_id起主导作用
如果每个分片只有一个块, 那就相当于是写操作平均分配到10个服务器上了

缺点, 有的块会死掉


## 控制数据分发 ##
1. 限制某些集合只能在某些分片上
2. 限制某些集合不能在某分片上

sh.addShardTag('分片1','高性能')
sh.addShardTag('分片2','低性能')

sh.addTagRange('msg.users',{'shardKey':MinKey},{'shardKey':MaxKey},'高性能')
这会导致msg.users的所有数据保存早高性能的分片上

sh.removeTagRange('分片1','高性能')

### 手动分片 ###
1. 关闭均衡器
	1. 连接到mongos
		```
			use config;
			db.settings.update({'_id':'balancer'},{'enabled:false'},true)
		```
2. 用db.chunks.find()找出每个分片的情况
3. sh.moveChunk('msg.users',{选择器},'分片1')
	1. 它的功能是将选择器选出的文档所在的块移动到分片1

# 第16章 分片管理 #
sh.status()查看集群状态, 会显示每个分片的块信息
config数据库存放了集群配置的相关信息, 一旦不要直接修改它
而是使用sh辅助变量

## shards ##
存放了分片的信息

## databases ##
显示每个数据库的信息, 比如这个数据库存在哪个分片, 是否启动划分功能

## collections ##
保存了分片集合的信息

## chunks ##
所有的块的信息

## changelog ##
集群的操作记录







#  #
插入
db.users.insert({对象});
db.users.batchInsert([{对象1},{对象2},...],{选项});

更新文档
update({修改条件},{修改器文档},是否upsert,是否multi)

查询
db.users.find({...},{控制字段的返回})
findOne
db.foo.find({'$where':func}) func是一个函数, 这个函数会以每一个文档为上下文进行调用, 返回true 即保留 返回false就放弃这个文档
游标的使用(类似迭代器, limit skip sort) 循环游标

删除文档
db.users.remove({删除条件})

删掉集合
db.users.drop()

数据库命令
db.runCommand({命令对象})


# 17 了解应用的动态 #
db.currentOp() 查看当前进行的操作
db.currentOp({'ns':'msg.users'})
db.killOp(123); 只有已经交出锁的操作还可以被停止
通常 update find remove都可以被停止

## 系统分析器 ##
分析器默认是关闭的 设置成0
db.setProfilingLevel(2) 打开分析器
db.setProfilingLevel(1,500) 只记录超过500毫秒的操作
db.system.profile.find().pretty()

启动的时候使用参数
--profile level
--slowms time
P304

## 计算空间消耗 ##
Object.bsonsize(obj)查看文档的大小

db.users.stats()
db.users.stats(1024*1024) 以MB为单位

db.stats() 查看数据库的大小


# 第18章 数据管理 #
## 配置身份验证 ##
安全检查默认是关闭的, 安全检查开启之后, 只有具有特定的权限的用户才能读写数据库
admin和local是两个特殊的数据库, 这两个数据库中的成员可以对任何数据库进行操作.
启动 mongoed 的时候 使用 --auth 启动安全检查
新版的mongodb的用户管理 http://www.jb51.net/article/53830.htm
```
创建一个超级用户
use admin
db.createUser({user: "adminUserName",pwd: "userPassword",roles:[{role: "userAdminAnyDatabase",db: "admin"}]})
可以切换到一个普通的数据库, 比如 msg 数据库
然后执行createUser语句, role可以是 read, readWrite (如果使用了不合法的role 就会提示错误)
```

db.createUser({user: "admin",pwd: "admin",roles:['root']})
db.changeUserPassword("username", "xxx")

```
登陆的时候
mongo --host xxx -u adminUserName -p userPassword --authenticationDatabase admin
```
```
查看当前用户
db.runCommand({usersInfo:"userName",showPrivileges:true})
```

db.runCommand(
  {
    updateUser:"username",
    pwd:"xxx",
    customData:{title:"xxx"}
  }
)
db.auth('user','pwd') 可以切换用户身份
在admin数据库建立用户之前 服务器上的本地客户端可以对数据库进行读写

分片时, admin数据库保存在配置服务器上, 因此分片中的mongod并不知道它的存在, 在这些mongod看来
他们虽然开启了身份验证但却不存在管理员用户 于是分片会允许一个本地的客户端无需身份验证便可读写数据.

## 建立和删除索引 ##
### 在单台服务器上建立索引, 挑选一个空闲时期 ###
db.users.ensureIndex({name:1},{background:true})
在前台创建索引将会导致数据库临时被锁定, 而后台不会(后台会定期释放锁), 但是后台耗时更长

### 在副本机上建立索引 ###
方法1:
1. 在主节点建立索引, 然后这个行为会被复制到其他的备份节点上
2. 如果数据量比较少, 那么不会有太大影响

方法2:
1. 关闭一个备份节点
2. 以单机模式启动它, 注意要改一下端口号
3. 在这服务器上建立索引
4. 重新加入副本集
5. 对其他备份节点做同样的操作
6. 此时还剩下主节点没做
7. 可以考虑把主节点降成备份节点 (通过stepDown 或 直接将主节点关了), 然后再如上操作; 或在后台对主节点创建索引

### 在分片集群上建立索引 ###
与副本集建立索引的步骤相同, 不过需要在每个分片上分别建立一次
1. 关闭均衡器
2. 对于每个分片, 可以认为它是一个副本集, 按照副本集建立索引的方法建立索引
3. 在mongos上执行ensureIndex
4. 启动均衡器

### 删除索引 ###
db.users.dropIndex(索引的名字 或 索引的配置)
db.users.dropIndexes()
索引的名字可以到system.indexes查找

## 预热数据 ##
重启机器 或 启动新的机器 会耗费一段时间让MongoDB将所有所需的数据从磁盘加载到内存.

## 压缩集合 ##
db.runCommand({compact:'users'})
可以指定paddingFactor:1.5 (最小1 最大4) 使得文档之间有间隔, 利于将来文档扩展
它会将集合的数据尽量往前移动, 但是mongodb占用的磁盘空间依然很大, 因为它并没有释放不用的空间
可以使用repair来回收磁盘空间, repair会对所有数据进行复制, 所以必须要有和当前数据文件大小一样的空余磁盘空间
P321

## 重命名集合 ##
db.users.renameCollection(...)



# 第20章 启动和停止MongoDB #
## 启动 ##
mongod
	--dbpath=数据存放的路径
	--port=端口 默认是27017
	--fork 使用fork创建子进程 在后台运行mongodb
	--logpath=... 如果启动fork 则必须指定这个选项
	-f / --config=指定配置文件
	
在配置文件里可以这么写
port = 1234
fork = true
logpath = ...
logappend = true
dbpath = ....

## 停止 ##
使用shutdown命令时, 会等待本分服务器同步完毕才真正关闭服务器.
```
use admin
db.shutdownServer()
相当于是db.adminCommand({'shutdown':1});
可以再添加 force : true 表示强制关闭, 这样相当于是发送了 kill 信号, 备份服务器可能没来得及完成同步
```

## 安全 ##
--bind_ip=... 最好将ip设置为一个内部的地址 以防止外部访问
--nohttpinterface mongodb启动时默认会启动一个微型http服务器
--rest 会启动一个简单的http界面 端口号+1000


## 日志 ##
默认输出到stdout
可以使用 --logpath=... 和 --logappend 改变这个行为
通过 -v -vv -vvv -vvvv -vvvvv来指定日志级别 级别越高 日志越详细 产生日志越多

值记录耗时超过500毫秒的查询记录
db.setProfilingLevel(1,500)
关闭分析器
db.setProfilingLevel(0)


# 第21章 监控MongoDB #
https://mms.10gen.com
MongoDB所使用的内存
1. 常驻内存, 查询文档时, 该文档所在的页就被载入内存中, 成为常驻内存的一部分
